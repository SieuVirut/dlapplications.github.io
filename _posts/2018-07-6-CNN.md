---
layout: post
title: Sự phát triển của CNN từ LeNet đến DenseNet
subtitle: từ architect đơn giản đến phức tạp
tags: [CNN]
math: true
hide = True
---
# Mở đầu .
Convolution neural network là một mạng neural được ứng dụng rất nhiều trong deep learning trong computer vision cho classifier và localizer
. Từ mạng CNN cơ bản người ta có thể tạo ra rất nhiều architect khác nhau, từ những mạng neural cơ bản 1 đến 2 layer đến 100 layer. Đã bao
giờ bạn tự hỏi nên sử dụng bao nhiêu layer, nên kết hợp conv với maxpooling thế nào? conv-maxpooling hay conv-conv-maxplooling ? hay nên sử dụng kernel 3x3 hay 5x5 thậm chí 7x7 điểm khác biệt là gì ? Làm gì khi model bị vanishing/exploxed gradient, hay tại sao thi thêm nhiều layer hơn thì theo lý thuyết accuarcy phải cao hơn so với shallow model, nhưng thực tế lại không phải accuarcy không giảm thậm chí là tăng đó có phải nguyên nhân do overfiting .Trong bài viết này ta sẽ tìm hiểu các architer nổi tiếng để xem cấu trúc của nó như thế nào, các ý tưởng về cnn mới nhất hiện nay .
# 1.	LeNet(1998).
LeNet là một trong những mạng CNN lâu đời nổi tiếng nhất được Yann LeCUn phát triển vào những năm 1998s. Cấu trúc của LeNet gồm 2 layer (Convolution + maxpooling) và 2 layer fully connection layer và output là softmax layer . Chúng ta cùng tìm hiểu chi tiết architecter của LeNet đối với dữ liệu mnist (accuracy lên đến 99%).
*	Input shape 28x28x3 
*	Layer 1 :
  *	 Convolution layer 1 : Kernel 5x5x3 , stride = 1,no padding, number filter = 6 ,output = 28x28x6.
  *	Maxpooling layer : pooling size 2x2,stride = 2,padding = “same”,output = 14x14x6.
*	Layer 2 :
  *	 Convolution layer 2 : kernel 5x5x6,stride = 1, no padding, number filter = 16,output = 10x10x16.
  *	Maxpooling layer : pooling size = 2x2, stride = 2, padding =”same”,output = 5x5x16.
*	Flatten output = 5x5x16 = 400
*	Fully connection  1 : output = 120
*	Fully connection  2 : output = 84
*	Softmax layer, output = 10 (10 digits).
Nhược điểm của LeNet là mạng còn rất đơn giản và sử dụng sigmoid (or tanh) ở mỗi convolution layer mạng tính toán rất chậm.
# 2. Alexnet(2012).
AlexNet là một mạng CNN đã dành chiến thắng trong cuộc thi ImageNet LSVRC-2012 năm 2012 với large margin (15.3% VS 26.2% error rates). AlexNet là một mạng CNN traning với một số lượng parameter rất lớn (60 million) so với LeNet. Một số đặc điểm:
  *	Sử dụng relu thay cho sigmoid(or tanh) để xử lý với non-linearity. Tăng tốc độ tính toán lên 6 lần.
  *	Sử dụng Dropout thay thế regularization để hạn chế overfitting. Tuy nhiên tốc độ tính toán giảm 2 lần với dropout rate là 0.5.
  *	Overlap pooling để giảm size của network ( Traditionally pooling regions không overlap).
  *	Sử dụng local response normalization để chuẩn hóa ở mỗi layer.
  *	Sử dụng kỹ thuật data augmentation để tạo them data training bằng cách translations, horizontal reflections.
  *	Alexnet training với 90 epochs trong 5 đến 6 ngày với 2 GTX 580 GPUs. Sử dụng SGD với learning rate 0.01, momentum 0.9 và weight decay 0.0005.
Architecter của Alexnet gồm 5 convolutional layer và 3 fully connection layer. Activation Relu được sử dụng sau mỗi convolution và fully connection layer. 
Detail architecter  với dataset là imagenet size là 227x227x3 với 1000 class ( khác với trong hình trên size là 224x224):
*	Input shape 227x227x3.
*	Layer 1 :
  *	Conv 1 : kernel : 11x11x3,stride = 4,no padding, number = 96,activation = relu,output = 55x55x96.
  *	 Maxpooling layer : pooling size = 3x3,stride = 2,padding =”same” ,output = 27x27x96.
  *	Normalize layer.
*	Layer 2 :
  *	Conv 2 : kernel :3x3x96,stride = 1, padding = “same”, number filter = 256,activation = relu,output = 27x27x256.
  *	Maxpooling layer : pooling size = 3x3,stride=2, padding =”same”,output = 13x13x256.
  *	Normalize layer.
*	Layer 3:
  *	Conv 3 : kernel :3x3x256, stride = 1,padding=”same”, number filter = 384, activation = relu, output = 13x13x384.
*	Layer 4:
  *	Conv 4 : kernel : 3x3x384 , stride = 1, padding = “same”, number filter = 384, activation= relu, output = 13x13x384
*	Layer 5 :
  *	Conv 5 : kernel 3x3x384, stride = 1, padding = “same”, number filter = 256, activation = relu, output = 13x13x256.
  *	 Pooling layer : pooling size = 3x3,stride =2,padding =”same”,output = 6x6x256.
*	Flatten 256x6x6 = 9216 
*	Fully connection layer 1 : activation = relu , output = 4096 + dropout(0.5).
*	Fully connection layer 2 : activation = relu , output = 4096 + dropout(0.5).
*	Fully connection layer 3 : activation = softmax , output = 1000 (number class)
# 3.	ZFNet(2013)
ZFNet là một mạng cnn thắng trong ILSVRC 2013 với top-5 error rate của 14.8% . ZFNet có cấu trúc rất giống với AlexNet với 5 layer convolution , 2 fully connected layer và 1 output softmax layer. Khác biệt ở chỗ kernel size ở mỗi Conv layer .Một số đặc điểm chính :
*	Tương tự AlexNet nhưng có một số điều chỉnh nhỏ.
*	Alexnet training trên 15m image trong khi ZF training chỉ có 1.3m image.
*	Sử dụng kernel 7x7 ở first layer (alexnet 11x11).Lý do là sử dụng kernel nhỏ hơn để giữ lại nhiều thông tin trên image hơn.
*	Tăng số lượng filter nhiều hơn so với alexnet
*	Training trên GTX 580 GPU trong 20 ngày

*	Input shape 224x224x3 .
*	Layer 1 :
  *	Conv 1 : kernel = 7x7x3, stride = 2, no padding, number filter = 96, output = 110x110x96.
  *	Maxpooling 1 : pooling size = 3x3,stride=2, padding = “same”,output = 55x55x96
  *	Normalize layer.
*	Layer 2 :
  *	Conv 2 : kernel = 5x5x96, stride = 2, no padding, number filter = 256, output = 26x26x256.
  *	Maxpooling 2 : pooling size = 3x3, stride=2,  padding = “same”,output = 13x13x256
  *	Normalize layer.
*	Layer 3:
  *	Conv 3 : kernel = 3x3x256, stride=1, padding=”same”, number filter = 384,output = 13x13x384.
  *	Layer 4 :
  *	Conv 4 : kernel = 3x3x384, stride=1, padding=”same”, number filter = 384,output = 13x13x384.
*	Layer 5 :
  *	Conv 5 : kernel = 3x3x384, stride=1, padding=”same”, number filter = 256,output = 13x13x256.
  *	Maxpooling  : pooling size = 3x3,stride =2,padding =”same”,output = 6x6x256.
*	Flatten 6x6x256 = 9216
*	Fully connection 1 : activation = relu,output =4096
*	Fully connection 2 : activation = relu,output =4096
*	Softmax layer for classifier ouput = 1000


